{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reddit API Test**\n",
    "\n",
    "* Add count of posts per subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append path to credentials\n",
    "sys.path.append('c:\\\\Users\\\\3leso\\\\Documents\\\\Elena\\\\Uni\\\\MasterThesis')\n",
    "from credentials import CLIENT_ID, CLIENT_SECRET, USER_AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Process-Lumpy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mothertoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emilheu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suspiciouspackages1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MidwestMonster2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775794</th>\n",
       "      <td>drizzystonks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775795</th>\n",
       "      <td>shinichiblue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775796</th>\n",
       "      <td>Estivenrex18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775797</th>\n",
       "      <td>TexasWithADollarsign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775798</th>\n",
       "      <td>boogaloo_guy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>775799 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        user\n",
       "0              Process-Lumpy\n",
       "1                mothertoker\n",
       "2                    emilheu\n",
       "3        Suspiciouspackages1\n",
       "4            MidwestMonster2\n",
       "...                      ...\n",
       "775794          drizzystonks\n",
       "775795          shinichiblue\n",
       "775796          Estivenrex18\n",
       "775797  TexasWithADollarsign\n",
       "775798          boogaloo_guy\n",
       "\n",
       "[775799 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = [\"Process-Lumpy\", \"mothertoker\", \"emilheu\", \"Suspiciouspackages1\", \"MidwestMonster2\"] \n",
    "\n",
    "users_all = pd.read_csv(\"output/users_all.csv\")\n",
    "users_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>339051</th>\n",
       "      <td>Roadman90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71685</th>\n",
       "      <td>Verbranding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254464</th>\n",
       "      <td>Master_REEEEEEEEEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483849</th>\n",
       "      <td>beesandtrees2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430762</th>\n",
       "      <td>boii</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      user\n",
       "339051           Roadman90\n",
       "71685          Verbranding\n",
       "254464  Master_REEEEEEEEEE\n",
       "483849       beesandtrees2\n",
       "430762                boii"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample\n",
    "user_sample = users_all.sample(n = 5, random_state=31)\n",
    "user_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "Do You Need to Handle Rate Limits When Using PRAW?\n",
    "\n",
    "No, you don't need to manually handle rate limits when using PRAW. PRAW automatically respects Reddit's API rate limits and will pause or retry requests as needed. However, understanding how PRAW manages rate limits and how Reddit enforces them is important, especially for large-scale data collection like your case with 776,000 users.\n",
    "How PRAW Handles Rate Limits\n",
    "\n",
    "    Automatic Handling:\n",
    "\n",
    "        PRAW automatically waits and retries when it encounters rate limit errors (e.g., \"You're doing that too much. Try again in X seconds\").\n",
    "\n",
    "        You can configure the ratelimit_seconds parameter to set the maximum time PRAW will wait before raising an exception. For example:\n",
    "\n",
    "        python\n",
    "        reddit = praw.Reddit(client_id=\"YOUR_CLIENT_ID\",\n",
    "                             client_secret=\"YOUR_CLIENT_SECRET\",\n",
    "                             user_agent=\"YOUR_USER_AGENT\",\n",
    "                             ratelimit_seconds=300)\n",
    "\n",
    "    Batch Requests:\n",
    "\n",
    "        PRAW often bundles multiple objects (e.g., submissions or comments) into a single request, which helps optimize API usage.\n",
    "\n",
    "    Rate Limit Information:\n",
    "\n",
    "        You can access rate limit details via reddit.auth.limits, which provides information such as remaining requests and reset timestamps.\n",
    "\n",
    "Reddit's API Rate Limits\n",
    "\n",
    "    Authenticated Requests (OAuth):\n",
    "\n",
    "        100 requests per minute per OAuth client ID.\n",
    "\n",
    "        Averaged over a 10-minute window, allowing bursts of up to 1,000 requests in 10 minutes\n",
    "\n",
    "    .\n",
    "\n",
    "Unauthenticated Requests:\n",
    "\n",
    "    Limited to 10 requests per minute\n",
    "\n",
    "    .\n",
    "\n",
    "Special Rate Limits:\n",
    "\n",
    "    Reddit may enforce additional limits for certain actions (e.g., commenting, banning users), which are not documented but handled by PRAW\n",
    "\n",
    "        .\n",
    "\n",
    "Handling 776,000 Users\n",
    "\n",
    "Given the scale of your task, here’s how you can efficiently collect data while staying within rate limits:\n",
    "Steps to Optimize Your Workflow\n",
    "\n",
    "    Use OAuth Authentication:\n",
    "\n",
    "        Ensure your app is authenticated with OAuth to get the higher rate limit (100 requests/minute).\n",
    "\n",
    "    Track Progress:\n",
    "\n",
    "        Use a counter to keep track of processed users and log progress periodically.\n",
    "\n",
    "    Parallel Processing:\n",
    "\n",
    "        If possible, split the task across multiple machines or threads using different OAuth client IDs to increase throughput.\n",
    "\n",
    "    Pause on Rate Limits:\n",
    "\n",
    "        Let PRAW handle rate limits automatically, but monitor reddit.auth.limits for real-time feedback on remaining requests.\n",
    "\n",
    "    Retry Logic:\n",
    "\n",
    "        Implement retry logic with exponential backoff if you encounter API errors or unexpected delays.\n",
    "\n",
    "Example Code for Large-Scale Data Collection\n",
    "\n",
    "Here’s a simplified example of how you might process users while respecting rate limits:\n",
    "\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Authenticate with Reddit API\n",
    "def authenticate():\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=CLIENT_ID[0],\n",
    "        client_secret=CLIENT_SECRET[0],\n",
    "        user_agent=USER_AGENT[0],\n",
    "        ratelimit_seconds=300\n",
    "    )\n",
    "    return reddit\n",
    "\n",
    "# Fetch subreddits for a user\n",
    "def fetch_user_subreddits(username, reddit):\n",
    "    try:\n",
    "        redditor = reddit.redditor(username)\n",
    "        subreddit_counts = {}\n",
    "        \n",
    "        # Fetch submissions\n",
    "        for submission in redditor.submissions.new(limit=None):\n",
    "            subreddit_name = submission.subreddit.display_name\n",
    "            if subreddit_name in subreddit_counts:\n",
    "                subreddit_counts[subreddit_name] += 1\n",
    "            else:\n",
    "                subreddit_counts[subreddit_name] = 1\n",
    "        \n",
    "        # Fetch comments\n",
    "        for comment in redditor.comments.new(limit=None):\n",
    "            subreddit_name = comment.subreddit.display_name\n",
    "            if subreddit_name in subreddit_counts:\n",
    "                subreddit_counts[subreddit_name] += 1\n",
    "            else:\n",
    "                subreddit_counts[subreddit_name] = 1\n",
    "\n",
    "        return subreddit_counts\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for user {username}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Process users in batches\n",
    "def process_users(user_list, reddit):\n",
    "    user_dict = {}\n",
    "    processed_count = 0\n",
    "    \n",
    "    for username in user_list:\n",
    "\n",
    "        subreddit_counts = fetch_user_subreddits(username, reddit)\n",
    "        user_dict[username] = subreddit_counts\n",
    "        # save\n",
    "        with open('output/user_data.json', 'w') as f:\n",
    "            json.dump(user_dict, f)\n",
    "        \n",
    "        # Log progress\n",
    "        processed_count += 1\n",
    "        print(f\"Processed {processed_count}/{len(user_list)} users.\")\n",
    "        \n",
    "        # Optional: Save results to file or database\n",
    "        \n",
    "        # Pause if needed (PRAW handles this automatically)\n",
    "        time.sleep(0)  # No explicit sleep required unless desired\n",
    "\n",
    "    return user_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for user Roadman90: received 403 HTTP response\n",
      "Processed 1/5 users.\n",
      "Processed 2/5 users.\n",
      "Processed 3/5 users.\n",
      "Processed 4/5 users.\n",
      "Processed 5/5 users.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    reddit = authenticate()\n",
    "    \n",
    "    # Example user list (replace with your actual list)\n",
    "    #user_list = [\"user1\", \"user2\", \"user3\", ...]\n",
    "    \n",
    "    process_users(user_sample['user'], reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('output/user_data.json','r') as f:\n",
    "    user_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Roadman90': [],\n",
       " 'Verbranding': {'EscapefromTarkov': 12,\n",
       "  'pathofexile': 22,\n",
       "  'Showerthoughts': 7,\n",
       "  'miamidolphins': 10,\n",
       "  'PcBuildHelp': 1,\n",
       "  'TeamfightTactics': 3,\n",
       "  'borderlands3': 6,\n",
       "  'afkarena': 3,\n",
       "  'AskReddit': 92,\n",
       "  'MagicArena': 3,\n",
       "  'ShittyLifeProTips': 1,\n",
       "  'DBZDokkanBattle': 8,\n",
       "  'Overwatch': 2,\n",
       "  'Yiddish': 2,\n",
       "  'i18n': 2,\n",
       "  'Diablo': 26,\n",
       "  'ContestOfChampionsLFG': 2,\n",
       "  'WildStar': 1,\n",
       "  'trees': 57,\n",
       "  'hearthstone': 6,\n",
       "  'videos': 13,\n",
       "  'keto': 11,\n",
       "  'inspiration': 1,\n",
       "  'Music': 15,\n",
       "  'funny': 9,\n",
       "  'gaming': 13,\n",
       "  'askscience': 7,\n",
       "  'leagueoflegends': 27,\n",
       "  'diablo3': 22,\n",
       "  'atheism': 4,\n",
       "  'PaymoneyWubby': 5,\n",
       "  'ContagiousLaughter': 1,\n",
       "  'AFKJourney': 2,\n",
       "  'billiards': 5,\n",
       "  'bourbon': 3,\n",
       "  'HolUp': 1,\n",
       "  'diablo4': 3,\n",
       "  'BlackPeopleTwitter': 1,\n",
       "  'JusticeServed': 1,\n",
       "  'conspiracy': 2,\n",
       "  'datingoverthirty': 10,\n",
       "  'freefolk': 1,\n",
       "  'buildapc': 2,\n",
       "  'xboxone': 1,\n",
       "  'RoastMe': 1,\n",
       "  'gifs': 3,\n",
       "  'ketogains': 4,\n",
       "  'DestinyTheGame': 1,\n",
       "  'bodybuilding': 8,\n",
       "  'pics': 13,\n",
       "  'steroids': 1,\n",
       "  'IAmA': 7,\n",
       "  'movies': 5,\n",
       "  'WTF': 7,\n",
       "  'worldnews': 1,\n",
       "  'Borderlands': 2,\n",
       "  'whatisthisthing': 1,\n",
       "  'nsfw': 2,\n",
       "  'politics': 4},\n",
       " 'Master_REEEEEEEEEE': {'StarWars': 3,\n",
       "  'AskReddit': 29,\n",
       "  'technology': 6,\n",
       "  'politics': 151,\n",
       "  'gaming': 19,\n",
       "  'gunpolitics': 26,\n",
       "  'worldnews': 13,\n",
       "  'liberalgunowners': 10,\n",
       "  'Libertarian': 45,\n",
       "  'Mordhau': 2,\n",
       "  'todayilearned': 8,\n",
       "  'MadeMeSmile': 1,\n",
       "  'NOWTTYG': 14,\n",
       "  'HydroHomies': 1,\n",
       "  'Damnthatsinteresting': 1,\n",
       "  'funny': 1,\n",
       "  'PoliticalHumor': 7,\n",
       "  'witcher': 5,\n",
       "  'latterdaysaints': 7,\n",
       "  'Futurology': 3,\n",
       "  'science': 2,\n",
       "  'CrappyDesign': 1,\n",
       "  'BlackPeopleTwitter': 4,\n",
       "  'Art': 1,\n",
       "  'Whatcouldgowrong': 2,\n",
       "  'trippinthroughtime': 1,\n",
       "  'StarWarsLeaks': 2,\n",
       "  'MarvelStudiosSpoilers': 2,\n",
       "  'WTF': 1,\n",
       "  'marvelstudios': 5,\n",
       "  'Bossfight': 2,\n",
       "  'DadReflexes': 1,\n",
       "  'JusticeServed': 2,\n",
       "  'pics': 7,\n",
       "  'news': 11,\n",
       "  'atheism': 3,\n",
       "  'trashy': 2,\n",
       "  'cyberpunkgame': 1,\n",
       "  'NatureIsFuckingLit': 1,\n",
       "  'teenagers': 1,\n",
       "  'The_Mueller': 1,\n",
       "  'nonononoyes': 1,\n",
       "  'gifs': 2,\n",
       "  'HumansBeingBros': 1,\n",
       "  'MovieDetails': 1,\n",
       "  'BikiniBottomTwitter': 2,\n",
       "  'space': 1,\n",
       "  'movies': 8,\n",
       "  'pcmasterrace': 5,\n",
       "  'dankmemes': 8,\n",
       "  'SonicTheHedgehog': 1,\n",
       "  'thanosdidnothingwrong': 1,\n",
       "  'mildlyinfuriating': 2,\n",
       "  'MensRights': 1,\n",
       "  'insanepeoplefacebook': 1,\n",
       "  'changemyview': 1,\n",
       "  'apexlegends': 6,\n",
       "  'prochoice': 11,\n",
       "  'AdviceAnimals': 3,\n",
       "  'iamatotalpieceofshit': 1,\n",
       "  'Cyberpunk': 1,\n",
       "  'UpliftingNews': 2,\n",
       "  'reddeadredemption': 1,\n",
       "  'dank_meme': 1,\n",
       "  'memes': 4,\n",
       "  'videos': 3,\n",
       "  'dankchristianmemes': 2,\n",
       "  'DunderMifflin': 2,\n",
       "  'lotrmemes': 1,\n",
       "  'titanfall': 4,\n",
       "  'rarepuppers': 1,\n",
       "  'buildapcsales': 2,\n",
       "  'battlestations': 1,\n",
       "  'WhitePeopleTwitter': 1,\n",
       "  'youseeingthisshit': 1,\n",
       "  'beards': 1,\n",
       "  'reactiongifs': 2,\n",
       "  'Music': 1,\n",
       "  'PrequelMemes': 2,\n",
       "  'KeanuBeingAwesome': 2,\n",
       "  'guncontrol': 1,\n",
       "  'Justfuckmyshitup': 1,\n",
       "  'MurderedByWords': 1,\n",
       "  'esist': 1,\n",
       "  'nostalgia': 1,\n",
       "  'oddlysatisfying': 2,\n",
       "  'Nicegirls': 1,\n",
       "  'youtube': 1,\n",
       "  'nottheonion': 1,\n",
       "  'OldSchoolCool': 1},\n",
       " 'beesandtrees2': {'dogs': 1,\n",
       "  'Mattress': 1,\n",
       "  'woodworking': 3,\n",
       "  'plantclinic': 2,\n",
       "  'plants': 1,\n",
       "  'homesecurity': 2,\n",
       "  'DIY': 12,\n",
       "  'greysanatomy': 47,\n",
       "  'PickAnAndroidForMe': 1,\n",
       "  'phones': 1,\n",
       "  'unpopularopinion': 47,\n",
       "  'homestead': 37,\n",
       "  'EatCheapAndHealthy': 24,\n",
       "  'NoStupidQuestions': 10,\n",
       "  'physicianassistant': 233,\n",
       "  'booksuggestions': 7,\n",
       "  'FirstTimeHomeBuyer': 72,\n",
       "  'RealEstate': 25,\n",
       "  'SuddenlyGay': 1,\n",
       "  'personalfinance': 3,\n",
       "  'PAstudent': 4,\n",
       "  'childfree': 41,\n",
       "  'LadyBoners': 4,\n",
       "  'antiMLM': 2,\n",
       "  'arizonatrail': 5,\n",
       "  'homeland': 8,\n",
       "  'insanepeoplefacebook': 19,\n",
       "  'Dexter': 9,\n",
       "  'CrohnsDisease': 31,\n",
       "  'AskReddit': 71,\n",
       "  'yurts': 11,\n",
       "  'rarepuppers': 1,\n",
       "  'melahomies': 9,\n",
       "  'Conservative': 25,\n",
       "  'zillowgonewild': 5,\n",
       "  'Yosemite': 39,\n",
       "  'Pyrography': 12,\n",
       "  'Outdoors': 2,\n",
       "  'cancer': 23,\n",
       "  'Trumpvirus': 11,\n",
       "  'politics': 45,\n",
       "  'TheWhiteLotusHBO': 8,\n",
       "  'Libertarian': 27,\n",
       "  'LifeProTips': 21,\n",
       "  'vaxxhappened': 4,\n",
       "  'antiwork': 7,\n",
       "  'Homesteading': 5,\n",
       "  'MadeMeSmile': 20,\n",
       "  'lastimages': 7,\n",
       "  '14ers': 13,\n",
       "  'AdviceAnimals': 12,\n",
       "  'medicine': 79,\n",
       "  'MedicalGore': 40,\n",
       "  'Anticonsumption': 7,\n",
       "  'TerrifyingAsFuck': 7,\n",
       "  'oddlyterrifying': 12,\n",
       "  'lastpodcastontheleft': 3,\n",
       "  'Damnthatsinteresting': 13,\n",
       "  'todayilearned': 18,\n",
       "  'YouShouldKnow': 25,\n",
       "  'Frugal': 6,\n",
       "  'Stepdadreflexes': 1,\n",
       "  'tinyhouse': 1,\n",
       "  'ExpectationVsReality': 1,\n",
       "  'tooktoomuch': 4,\n",
       "  'Scrubs': 5,\n",
       "  'Economics': 8,\n",
       "  'MapPorn': 14,\n",
       "  'LateStageCapitalism': 26,\n",
       "  'WelcomeToGilead': 1,\n",
       "  'democrats': 8,\n",
       "  'BuyItForLife': 4,\n",
       "  'Everest': 1,\n",
       "  'climbing': 7,\n",
       "  'facepalm': 14,\n",
       "  'StrangerThings': 1,\n",
       "  'dataisbeautiful': 5,\n",
       "  'nextfuckinglevel': 32,\n",
       "  'interestingasfuck': 42,\n",
       "  'California': 13,\n",
       "  'nottheonion': 11,\n",
       "  'wyoming': 5,\n",
       "  'caving': 15,\n",
       "  'bears': 2,\n",
       "  'CozyPlaces': 1,\n",
       "  'explainlikeimfive': 1,\n",
       "  'awfuleverything': 13,\n",
       "  'HydroHomies': 3,\n",
       "  'science': 11,\n",
       "  'JoeBiden': 12,\n",
       "  'trashy': 3,\n",
       "  'RedditForGrownups': 7,\n",
       "  'TheLastAirbender': 4,\n",
       "  'sustainability': 3,\n",
       "  'Money': 1,\n",
       "  'netflix': 1,\n",
       "  'Futurology': 6,\n",
       "  'Scams': 7,\n",
       "  'medizzy': 17,\n",
       "  'lifehacks': 3,\n",
       "  'illnessfakers': 3,\n",
       "  'popping': 10,\n",
       "  'MealPrepSunday': 1,\n",
       "  'lostgeneration': 11,\n",
       "  'IASIP': 3,\n",
       "  'iamatotalpieceofshit': 10,\n",
       "  'leanfire': 6,\n",
       "  'electricians': 3,\n",
       "  'Tinder': 2,\n",
       "  'arresteddevelopment': 1,\n",
       "  'financialindependence': 4,\n",
       "  'BikiniBottomTwitter': 2,\n",
       "  'toastme': 7,\n",
       "  'food': 1,\n",
       "  'woodburning': 4,\n",
       "  'UniversalHealthCare': 1,\n",
       "  'snowshoeing': 5,\n",
       "  'ATLA': 5,\n",
       "  'funny': 22,\n",
       "  'fakealbumcovers': 1,\n",
       "  'WitchesVsPatriarchy': 2,\n",
       "  'ThePatient': 1,\n",
       "  'Kombucha': 2,\n",
       "  'ShitAmericansSay': 3,\n",
       "  'HandmaidsTaleShow': 1,\n",
       "  'RoastMe': 3,\n",
       "  'TargetedShirts': 1,\n",
       "  'Beekeeping': 5,\n",
       "  'WhyWomenLiveLonger': 1,\n",
       "  'selfreliance': 1,\n",
       "  'Botchedsurgeries': 9,\n",
       "  'Melanoma': 2,\n",
       "  'UrbanHell': 3,\n",
       "  'oddlysatisfying': 5,\n",
       "  'Persecutionfetish': 1,\n",
       "  'ABoringDystopia': 12,\n",
       "  'MurderedByWords': 1,\n",
       "  'agedlikemilk': 1,\n",
       "  'thalassophobia': 1,\n",
       "  'MarchAgainstNazis': 5,\n",
       "  'prochoice': 12,\n",
       "  'whitecoatinvestor': 1,\n",
       "  'ladyladyboners': 1,\n",
       "  'PatriotTV': 5,\n",
       "  'WhitePeopleTwitter': 10,\n",
       "  'Instagramreality': 2,\n",
       "  'progresspics': 2,\n",
       "  'TheRightCantMeme': 11,\n",
       "  'auntienetwork': 1,\n",
       "  'EarthPorn': 3,\n",
       "  'TopMindsOfReddit': 12,\n",
       "  'IdiotsInCars': 1,\n",
       "  'HistoryMemes': 1,\n",
       "  'PoliticalHumor': 36,\n",
       "  'WeWantPlates': 2,\n",
       "  'WayOfTheBern': 1,\n",
       "  'FuckYouKaren': 3,\n",
       "  'serialkillers': 3,\n",
       "  'insaneparents': 2,\n",
       "  'EntitledBitch': 4,\n",
       "  'thepromisedneverland': 1,\n",
       "  'CovIdiots': 5,\n",
       "  'ShittyLifeProTips': 3,\n",
       "  'natureismetal': 3,\n",
       "  'OuterBanksNetflix': 6,\n",
       "  'Permaculture': 1,\n",
       "  'videos': 1,\n",
       "  'stopsmoking': 1,\n",
       "  'peopleofwalmart': 1,\n",
       "  'QAnonCasualties': 3,\n",
       "  'FullmetalAlchemist': 1,\n",
       "  'CrackheadCraigslist': 2,\n",
       "  'badwomensanatomy': 3,\n",
       "  'spotify': 1,\n",
       "  'FuckCaillou': 1,\n",
       "  'wallstreetbets': 1,\n",
       "  'HistoryPorn': 4,\n",
       "  'AmericanFascism2020': 5,\n",
       "  'PenmanshipPorn': 1,\n",
       "  'CapitolConsequences': 1,\n",
       "  'rickandmorty': 1,\n",
       "  'DiWHY': 1,\n",
       "  'AskTrumpSupporters': 3,\n",
       "  'OldSchoolCool': 3,\n",
       "  'wedding': 1,\n",
       "  'medical': 20,\n",
       "  'The_Dennis': 1,\n",
       "  'WatchPeopleDieInside': 5,\n",
       "  'GetOutOfBed': 1,\n",
       "  'Awwducational': 1,\n",
       "  'Letterkenny': 2,\n",
       "  'Canning': 1,\n",
       "  'SBSK': 1,\n",
       "  'fakehistoryporn': 1,\n",
       "  'photoshopbattles': 1,\n",
       "  'UnethicalLifeProTips': 1,\n",
       "  'GTAorRussia': 2,\n",
       "  'hamiltonmusical': 1,\n",
       "  'barndominiums': 1,\n",
       "  'TheWire': 1,\n",
       "  'aww': 4,\n",
       "  'ToiletPaperUSA': 2,\n",
       "  'woahdude': 1,\n",
       "  'AskOuija': 1,\n",
       "  'pics': 2,\n",
       "  'AnimalsOnReddit': 1},\n",
       " 'boii': {'rccars': 2,\n",
       "  'eFreebies': 4,\n",
       "  'nexus4': 4,\n",
       "  'androidapps': 6,\n",
       "  'frugalmalefashion': 2,\n",
       "  'GTAA': 3,\n",
       "  'Korean': 27,\n",
       "  'SJSU': 3,\n",
       "  'spotted': 2,\n",
       "  'fullmoviesonyoutube': 1,\n",
       "  'kpop': 41,\n",
       "  'crappymusic': 1,\n",
       "  'imports': 1,\n",
       "  'pics': 18,\n",
       "  'kpics': 2,\n",
       "  'yester': 3,\n",
       "  'AndroidQuestions': 4,\n",
       "  'malefashionadvice': 26,\n",
       "  'answers': 8,\n",
       "  'HomeworkHelp': 6,\n",
       "  'grammar': 10,\n",
       "  'videos': 13,\n",
       "  'AdviceAnimals': 5,\n",
       "  'funny': 16,\n",
       "  'omegle': 2,\n",
       "  'Scholar': 2,\n",
       "  'bayarea': 8,\n",
       "  'korea': 3,\n",
       "  'WTF': 17,\n",
       "  'Doesthisexist': 2,\n",
       "  'DoesAnybodyElse': 43,\n",
       "  'techsupport': 5,\n",
       "  'tipofmytongue': 17,\n",
       "  'happy': 1,\n",
       "  'jobs': 2,\n",
       "  'freebies': 6,\n",
       "  'askscience': 8,\n",
       "  'IWantToLearn': 13,\n",
       "  'firstworldproblems': 3,\n",
       "  'yourmom': 1,\n",
       "  'AskReddit': 65,\n",
       "  'ForeverAlone': 1,\n",
       "  'offmychest': 1,\n",
       "  'reddit.com': 6,\n",
       "  'hiphopheads': 3,\n",
       "  'youtube': 2,\n",
       "  'offbeat': 1,\n",
       "  'Money': 1,\n",
       "  'nonononoyes': 1,\n",
       "  'HongKong': 1,\n",
       "  'spiders': 1,\n",
       "  'MadeMeSmile': 2,\n",
       "  'anker': 1,\n",
       "  'u_reddit_irl': 1,\n",
       "  'PokemonGoFriends': 1,\n",
       "  'popping': 2,\n",
       "  'ConvenientCop': 1,\n",
       "  'holdmyfeedingtube': 6,\n",
       "  'nextfuckinglevel': 4,\n",
       "  'blender': 1,\n",
       "  'MorbidReality': 1,\n",
       "  'FreeGameFindings': 5,\n",
       "  'Unexpected': 4,\n",
       "  'HolUp': 2,\n",
       "  'FuckYouKaren': 1,\n",
       "  'Roadcam': 3,\n",
       "  'GTAorRussia': 2,\n",
       "  'PublicFreakout': 3,\n",
       "  'DiscountedProducts': 1,\n",
       "  'subaru': 2,\n",
       "  'JusticeServed': 2,\n",
       "  'ThatsInsane': 2,\n",
       "  'gifs': 1,\n",
       "  'tifu': 3,\n",
       "  'watchpeoplesurvive': 1,\n",
       "  'casualiama': 2,\n",
       "  'rareinsults': 1,\n",
       "  'dadjokes': 1,\n",
       "  'mildlyinteresting': 3,\n",
       "  'Showerthoughts': 2,\n",
       "  'Whatcouldgowrong': 1,\n",
       "  'nevertellmetheodds': 1,\n",
       "  'KidsAreFuckingStupid': 1,\n",
       "  'gifsthatkeepongiving': 2,\n",
       "  'AskHR': 1,\n",
       "  'oddlysatisfying': 1,\n",
       "  'aww': 2,\n",
       "  'CatastrophicFailure': 2,\n",
       "  'uglyduckling': 1,\n",
       "  'WhyWereTheyFilming': 2,\n",
       "  'SweatyPalms': 1,\n",
       "  'LifeProTips': 1,\n",
       "  'niceguys': 2,\n",
       "  'Wellthatsucks': 3,\n",
       "  'Android': 1,\n",
       "  'tmobile': 1,\n",
       "  'worldnews': 2,\n",
       "  'technology': 2,\n",
       "  'GrandTheftAutoV': 2,\n",
       "  'JusticePorn': 1,\n",
       "  'GTAVadventures': 1,\n",
       "  'howto': 1,\n",
       "  'GetEmployed': 3,\n",
       "  'shittingadvice': 1,\n",
       "  'gaming': 2,\n",
       "  'science': 1,\n",
       "  'self': 1,\n",
       "  'politics': 1,\n",
       "  'IAmA': 1,\n",
       "  'fffffffuuuuuuuuuuuu': 2,\n",
       "  'woahdude': 1,\n",
       "  'circlejerk': 6,\n",
       "  'Frugal': 1,\n",
       "  'shittyadvice': 11,\n",
       "  'apathy': 1,\n",
       "  'DAE': 1}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "Key Considerations\n",
    "\n",
    "    Time Estimate: At 100 requests/minute, processing 776,000 users would take approximately 129 hours (if each user requires one request). Parallelization can reduce this significantly.\n",
    "\n",
    "    Ethical Compliance: Ensure you're collecting only publicly available data and adhering to Reddit's API terms of use.\n",
    "\n",
    "    Monitoring: Use logging or monitoring tools to track progress and detect issues during long-running tasks.\n",
    "\n",
    "PRAW's automatic rate limit handling makes it well-suited for large-scale data collection tasks like yours!\n",
    "\n",
    "***\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
