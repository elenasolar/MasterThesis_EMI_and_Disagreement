{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LLama 3.1-8B (base version)**\n",
    "\n",
    "Code was adapted based on an example from [Link to Source](https://github.com/adidror005/youtube-videos/blob/main/old_videos/LLAMA_3_Fine_Tuning_for_Sequence_Classification_Actual_Video.ipynb)\n",
    "\n",
    "## **2 Labels only, Scale (Logits/Probability)**\n",
    "\n",
    "## **LoRa for Attention and FFN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Llama**\n",
    "\n",
    "\n",
    "### Big Picture Overview of Parameter Efficient Fine Tuning Methods like LoRA and QLoRA Fine Tuning for Sequence Classification\n",
    "\n",
    "**The Essence of Fine-tuning**\n",
    "- LLMs are pre-trained on vast amounts of data for broad language understanding.\n",
    "- Fine-tuning is crucial for specializing in specific domains or tasks, involving adjustments with smaller, relevant datasets.\n",
    "\n",
    "**Model Fine-tuning with PEFT: Exploring LoRA and QLoRA**\n",
    "- Traditional fine-tuning is resource-intensive; PEFT (Parameter Efficient Fine-tuning) makes the process faster and less demanding.\n",
    "- Focus on two PEFT methods: LoRA and QLoRA.\n",
    "\n",
    "**The Power of PEFT**\n",
    "- PEFT modifies only a subset of the LLM's parameters, enhancing speed and reducing memory demands, making it suitable for less powerful devices.\n",
    "\n",
    "**LoRA: Efficiency through Adapters**\n",
    "- **Low-Rank Adaptation (LoRA):** Injects small trainable adapters into the pre-trained model.\n",
    "- **Equation:** For a weight matrix $W$, LoRA approximates $W = W_0 + BA$, where $W_0$ is the original weight matrix, and $BA$ represents the low-rank modification through trainable matrices $B$ and $A$.\n",
    "- Adapters learn task nuances while keeping the majority of the LLM unchanged, minimizing overhead.\n",
    "\n",
    "**QLoRA: Compression and Speed**\n",
    "- **Quantized LoRA (QLoRA):** Extends LoRA by quantizing the model’s weights, further reducing size and enhancing speed.\n",
    "- **Innovations in QLoRA:**\n",
    "  1. **4-bit Quantization:** Uses a 4-bit data type, NormalFloat (NF4), for optimal weight quantization, drastically reducing memory usage.\n",
    "  2. **Low-Rank Adapters:** Fine-tuned with 16-bit precision to effectively capture task-specific nuances.\n",
    "  3. **Double Quantization:** Reduces quantization constants from 32-bit to 8-bit, saving additional memory without accuracy loss.\n",
    "  4. **Paged Optimizers:** Manages memory efficiently during training, optimizing for large tasks.\n",
    "\n",
    "**Why PEFT Matters**\n",
    "- **Rapid Learning:** Speeds up model adaptation.\n",
    "- **Smaller Footprint:** Eases deployment with reduced model size.\n",
    "- **Edge-Friendly:** Fits better on devices with limited resources, enhancing accessibility.\n",
    "\n",
    "**Conclusion**\n",
    "- PEFT methods like LoRA and QLoRA revolutionize LLM fine-tuning by focusing on efficiency, facilitating faster adaptability, smaller models, and broader device compatibility.\n",
    "\n",
    "***\n",
    "\n",
    "### Fine-tuning for Sentiment Analysis Classification:\n",
    "\n",
    "\n",
    "#### 1. Text Generation with Sentiment Label as part of text\n",
    "- **Approach**: Train the model to generate text that naturally appends the sentiment label at the end.\n",
    "- **Input**: \"TSLA slashes model Y prices ======\"\n",
    "- **Output**: \"TSLA slashes model Y prices ====== Bearish\"\n",
    "- **Use Case**: This method is useful for applications requiring continuous text output that includes embedded sentiment analysis, such as interactive chatbots or automated content creation tools.\n",
    "\n",
    "\n",
    "#### 2. Sequence Classification Head\n",
    "- **Approach**: Add a sequence classification head (linear layer) on top of the LLaMa Model transformer. This setup is similar to GPT-2 and focuses on classifying the sentiment based on the last relevant token in the sequence.\n",
    "    - **Token Positioning**:\n",
    "        - **With pad_token_id**: The model identifies and ignores padding tokens, using the last non-padding token for classification.\n",
    "        - **Without pad_token_id**: It defaults to the last token in each sequence.\n",
    "        - **inputs_embeds**: If embeddings are directly passed (without input_ids), the model cannot identify padding tokens and takes the last embedding in each sequence as the input for classification.\n",
    "- **Input**: Specific sentences (e.g., \"TSLA slashes Model Y prices\").\n",
    "- **Output**: Direct sentiment classification (e.g., \"Bearish\").\n",
    "- **Training Objective**: Minimize cross-entropy loss between the predicted and the actual sentiment labels.\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/model_doc/llama\n",
    "\n",
    "### Peft Configs\n",
    "* Bits and bytes config for quantization\n",
    "* Lora config for lora\n",
    "\n",
    "### Going to use Hugginface Transformers trainer class: Main componenents\n",
    "* Hugging face dataset (for train + eval)\n",
    "* Data collater\n",
    "* Compute Metrics\n",
    "* Class weights since we use custom trainer and also custom weighted loss..\n",
    "* trainingArgs: like # epochs, learning rate, weight decay etc..\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting torch<3,>=2.0 (from bitsandbytes)\n",
      "  Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from bitsandbytes) (1.24.4)\n",
      "Collecting filelock (from torch<3,>=2.0->bitsandbytes)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (2023.9.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.3)\n",
      "Using cached bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: triton, nvidia-cusparselt-cu12, typing-extensions, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, bitsandbytes\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "Successfully installed bitsandbytes-0.45.5 filelock-3.18.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0 typing-extensions-4.13.2\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.51.2-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.30.2 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.2\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2023.9.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2023.7.22)\n",
      "Using cached accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.6.0\n",
      "Collecting peft\n",
      "  Using cached peft-0.15.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from peft) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.11/site-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (from peft) (4.51.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from peft) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from peft) (1.6.0)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /opt/conda/lib/python3.11/site-packages (from peft) (0.30.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (2023.9.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (4.13.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2023.7.22)\n",
      "Using cached peft-0.15.1-py3-none-any.whl (411 kB)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.15.1\n",
      "Collecting trl\n",
      "  Using cached trl-0.16.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /opt/conda/lib/python3.11/site-packages (from trl) (1.6.0)\n",
      "Collecting datasets>=3.0.0 (from trl)\n",
      "  Using cached datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting rich (from trl)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: transformers>=4.46.0 in /opt/conda/lib/python3.11/site-packages (from trl) (4.51.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets>=3.0.0->trl) (3.18.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=3.0.0->trl)\n",
      "  Using cached pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets>=3.0.0->trl) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets>=3.0.0->trl) (2.1.1)\n",
      "Collecting requests>=2.32.2 (from datasets>=3.0.0->trl)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets>=3.0.0->trl)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets>=3.0.0->trl)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=3.0.0->trl)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0.0->trl) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets>=3.0.0->trl) (3.8.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.46.0->trl) (0.21.1)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->trl)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->trl) (2.16.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.13.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->trl)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=3.0.0->trl)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2023.7.22)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets>=3.0.0->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets>=3.0.0->trl) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets>=3.0.0->trl) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (2.1.3)\n",
      "Using cached trl-0.16.1-py3-none-any.whl (336 kB)\n",
      "Using cached datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: xxhash, tqdm, requests, pyarrow, mdurl, dill, multiprocess, markdown-it-py, rich, datasets, trl\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 13.0.0\n",
      "    Uninstalling pyarrow-13.0.0:\n",
      "      Successfully uninstalled pyarrow-13.0.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "Successfully installed datasets-3.5.0 dill-0.3.8 markdown-it-py-3.0.0 mdurl-0.1.2 multiprocess-0.70.16 pyarrow-19.0.1 requests-2.32.3 rich-14.0.0 tqdm-4.67.1 trl-0.16.1 xxhash-3.5.0\n",
      "Collecting pyarrow==18.1.0\n",
      "  Using cached pyarrow-18.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Using cached pyarrow-18.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "Installing collected packages: pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 19.0.1\n",
      "    Uninstalling pyarrow-19.0.1:\n",
      "      Successfully uninstalled pyarrow-19.0.1\n",
      "Successfully installed pyarrow-18.1.0\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from evaluate) (1.24.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.11/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from evaluate) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.11/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.11/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.9.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.8.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "Collecting wandb\n",
      "  Using cached wandb-0.19.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (3.1.40)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.11/site-packages (from wandb) (3.11.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (4.24.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (5.9.5)\n",
      "Collecting pydantic<3 (from wandb)\n",
      "  Using cached pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Using cached sentry_sdk-2.25.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Using cached setproctitle-1.3.5-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /opt/conda/lib/python3.11/site-packages (from wandb) (4.13.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic<3->wandb)\n",
      "  Using cached pydantic_core-2.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3->wandb)\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (3.0.5)\n",
      "Using cached wandb-0.19.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.9 MB)\n",
      "Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Using cached pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Using cached pydantic_core-2.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached sentry_sdk-2.25.1-py2.py3-none-any.whl (339 kB)\n",
      "Using cached setproctitle-1.3.5-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, setproctitle, sentry-sdk, pydantic-core, docker-pycreds, annotated-types, pydantic, wandb\n",
      "Successfully installed annotated-types-0.7.0 docker-pycreds-0.4.0 pydantic-2.11.3 pydantic-core-2.33.1 sentry-sdk-2.25.1 setproctitle-1.3.5 typing-inspection-0.4.0 wandb-0.19.9\n",
      "Collecting adapter-transformers\n",
      "  Using cached adapter_transformers-4.0.0-py3-none-any.whl\n",
      "Collecting adapters (from adapter-transformers)\n",
      "  Using cached adapters-1.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers~=4.47.1 (from adapters->adapter-transformers)\n",
      "  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from adapters->adapter-transformers) (23.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers~=4.47.1->adapters->adapter-transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.11/site-packages (from transformers~=4.47.1->adapters->adapter-transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers~=4.47.1->adapters->adapter-transformers) (1.24.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers~=4.47.1->adapters->adapter-transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers~=4.47.1->adapters->adapter-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers~=4.47.1->adapters->adapter-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers~=4.47.1->adapters->adapter-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers~=4.47.1->adapters->adapter-transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers~=4.47.1->adapters->adapter-transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers~=4.47.1->adapters->adapter-transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers~=4.47.1->adapters->adapter-transformers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers~=4.47.1->adapters->adapter-transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers~=4.47.1->adapters->adapter-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers~=4.47.1->adapters->adapter-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers~=4.47.1->adapters->adapter-transformers) (2023.7.22)\n",
      "Using cached adapters-1.1.0-py3-none-any.whl (293 kB)\n",
      "Using cached transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "Installing collected packages: transformers, adapters, adapter-transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.2\n",
      "    Uninstalling transformers-4.51.2:\n",
      "      Successfully uninstalled transformers-4.51.2\n",
      "Successfully installed adapter-transformers-4.0.0 adapters-1.1.0 transformers-4.47.1\n"
     ]
    }
   ],
   "source": [
    "# install packages\n",
    "!pip install -U bitsandbytes\n",
    "!pip install -U transformers\n",
    "!pip install -U accelerate\n",
    "!pip install -U peft\n",
    "!pip install -U trl\n",
    "!pip install pyarrow==18.1.0\n",
    "!pip install evaluate\n",
    "!pip install --upgrade wandb\n",
    "!pip install adapter-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import evaluate\n",
    "import functools # ??\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, PeftConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "\n",
    "import transformers\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                        AutoTokenizer,\n",
    "                        AutoModel,\n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                            Trainer,\n",
    "                            DataCollatorWithPadding,\n",
    "                          pipeline, \n",
    "                          logging)\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix,\n",
    "                            f1_score, balanced_accuracy_score)\n",
    "\n",
    "from peft import PeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.is_available()\n",
    "#torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Authenticate for Hugging Face**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging face access\n",
    "\n",
    "from huggingface_hub import login\n",
    "with open(\"../../../../login/hf_key.txt\", 'r') as f: \n",
    "    HF_TOKEN = str(f.read())\n",
    "    \n",
    "login(token = HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33melena-solar\u001b[0m (\u001b[33melena-solar-university-of-konstanz\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb\n",
    "\n",
    "with open(\"../../../../login/wandb.txt\", 'r') as f: \n",
    "    WB_TOKEN = str(f.read())\n",
    "\n",
    "wandb.login(key=WB_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_parent</th>\n",
       "      <th>body_child</th>\n",
       "      <th>msg_id_parent</th>\n",
       "      <th>msg_id_child</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>datetime</th>\n",
       "      <th>exact_time</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>I live in rural Saskatchewan, Canada. We have ...</td>\n",
       "      <td>I'm in NE USA we've had 3 in two years...all e...</td>\n",
       "      <td>cnddov1</td>\n",
       "      <td>cndj2gv</td>\n",
       "      <td>climate</td>\n",
       "      <td>03/01/2015 23:18</td>\n",
       "      <td>1420327135</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>I live in rural Saskatchewan, Canada. We have ...</td>\n",
       "      <td>One hundred year flood just means a one in one...</td>\n",
       "      <td>cnddov1</td>\n",
       "      <td>cndkpy7</td>\n",
       "      <td>climate</td>\n",
       "      <td>04/01/2015 00:10</td>\n",
       "      <td>1420330231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Convince her of what? That it's happening or t...</td>\n",
       "      <td>That anthropocentric climate change is actuall...</td>\n",
       "      <td>cndnlrd</td>\n",
       "      <td>cndnsxt</td>\n",
       "      <td>climate</td>\n",
       "      <td>04/01/2015 01:45</td>\n",
       "      <td>1420335952</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disagree</td>\n",
       "      <td>I think this prediction is about as valid as s...</td>\n",
       "      <td>It's January. Literally no one said it would b...</td>\n",
       "      <td>cndl5x4</td>\n",
       "      <td>cndybsy</td>\n",
       "      <td>climate</td>\n",
       "      <td>04/01/2015 08:01</td>\n",
       "      <td>1420358465</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disagree</td>\n",
       "      <td>Mann hasn't been honest in decades, so I'm cur...</td>\n",
       "      <td>There have been a dozen re-constructions of Ma...</td>\n",
       "      <td>cne462t</td>\n",
       "      <td>cne89ej</td>\n",
       "      <td>climate</td>\n",
       "      <td>04/01/2015 17:45</td>\n",
       "      <td>1420393544</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42838</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Not trying to spark an argument but a legitima...</td>\n",
       "      <td>Keeping in mind that the Palestinians killed m...</td>\n",
       "      <td>gyo197v</td>\n",
       "      <td>gyotff1</td>\n",
       "      <td>Republican</td>\n",
       "      <td>19/05/2021 12:36</td>\n",
       "      <td>1621427788</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42839</th>\n",
       "      <td>agree</td>\n",
       "      <td>Y'all saw Guilianis hail Mary right? Get his s...</td>\n",
       "      <td>Same I want these assholes in jail, full stop....</td>\n",
       "      <td>gynfsu4</td>\n",
       "      <td>gyp3u39</td>\n",
       "      <td>democrats</td>\n",
       "      <td>19/05/2021 13:56</td>\n",
       "      <td>1621432578</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42840</th>\n",
       "      <td>agree</td>\n",
       "      <td>Why don't I see ads holding Republicans accoun...</td>\n",
       "      <td>Yeah, I agree with the goal of this post but n...</td>\n",
       "      <td>gyn6nzm</td>\n",
       "      <td>gyp5vzw</td>\n",
       "      <td>democrats</td>\n",
       "      <td>19/05/2021 14:11</td>\n",
       "      <td>1621433471</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42841</th>\n",
       "      <td>agree</td>\n",
       "      <td>How about ... no? This is strange. Community o...</td>\n",
       "      <td>I know, it feels strange too. We wouldn't hold...</td>\n",
       "      <td>gyp71o7</td>\n",
       "      <td>gyp7en6</td>\n",
       "      <td>BlackLivesMatter</td>\n",
       "      <td>19/05/2021 14:21</td>\n",
       "      <td>1621434116</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42842</th>\n",
       "      <td>agree</td>\n",
       "      <td>Can you recruit some white allies so you aren'...</td>\n",
       "      <td>Ooooo, now I like the taking the day request. ...</td>\n",
       "      <td>gyp7b89</td>\n",
       "      <td>gyp7vjv</td>\n",
       "      <td>BlackLivesMatter</td>\n",
       "      <td>19/05/2021 14:25</td>\n",
       "      <td>1621434314</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42843 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                        body_parent  \\\n",
       "0       neutral  I live in rural Saskatchewan, Canada. We have ...   \n",
       "1       neutral  I live in rural Saskatchewan, Canada. We have ...   \n",
       "2       neutral  Convince her of what? That it's happening or t...   \n",
       "3      disagree  I think this prediction is about as valid as s...   \n",
       "4      disagree  Mann hasn't been honest in decades, so I'm cur...   \n",
       "...         ...                                                ...   \n",
       "42838   neutral  Not trying to spark an argument but a legitima...   \n",
       "42839     agree  Y'all saw Guilianis hail Mary right? Get his s...   \n",
       "42840     agree  Why don't I see ads holding Republicans accoun...   \n",
       "42841     agree  How about ... no? This is strange. Community o...   \n",
       "42842     agree  Can you recruit some white allies so you aren'...   \n",
       "\n",
       "                                              body_child msg_id_parent  \\\n",
       "0      I'm in NE USA we've had 3 in two years...all e...       cnddov1   \n",
       "1      One hundred year flood just means a one in one...       cnddov1   \n",
       "2      That anthropocentric climate change is actuall...       cndnlrd   \n",
       "3      It's January. Literally no one said it would b...       cndl5x4   \n",
       "4      There have been a dozen re-constructions of Ma...       cne462t   \n",
       "...                                                  ...           ...   \n",
       "42838  Keeping in mind that the Palestinians killed m...       gyo197v   \n",
       "42839  Same I want these assholes in jail, full stop....       gynfsu4   \n",
       "42840  Yeah, I agree with the goal of this post but n...       gyn6nzm   \n",
       "42841  I know, it feels strange too. We wouldn't hold...       gyp71o7   \n",
       "42842  Ooooo, now I like the taking the day request. ...       gyp7b89   \n",
       "\n",
       "      msg_id_child         subreddit          datetime  exact_time  target  \n",
       "0          cndj2gv           climate  03/01/2015 23:18  1420327135       1  \n",
       "1          cndkpy7           climate  04/01/2015 00:10  1420330231       1  \n",
       "2          cndnsxt           climate  04/01/2015 01:45  1420335952       1  \n",
       "3          cndybsy           climate  04/01/2015 08:01  1420358465       0  \n",
       "4          cne89ej           climate  04/01/2015 17:45  1420393544       0  \n",
       "...            ...               ...               ...         ...     ...  \n",
       "42838      gyotff1        Republican  19/05/2021 12:36  1621427788       1  \n",
       "42839      gyp3u39         democrats  19/05/2021 13:56  1621432578       2  \n",
       "42840      gyp5vzw         democrats  19/05/2021 14:11  1621433471       2  \n",
       "42841      gyp7en6  BlackLivesMatter  19/05/2021 14:21  1621434116       2  \n",
       "42842      gyp7vjv  BlackLivesMatter  19/05/2021 14:25  1621434314       2  \n",
       "\n",
       "[42843 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the data\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"../../../../data/debagree_new_preprocessing_com_rep.csv\")\n",
    "data = data[[\"label\", \"body_parent\", \"body_child\", \"msg_id_parent\", \"msg_id_child\", \"subreddit\", \"datetime\", \"exact_time\"]].sort_values(by = \"exact_time\").reset_index(drop = True)\n",
    "\n",
    "# keep integer labels\n",
    "data['target'] = data['label']\n",
    "\n",
    "# for readability, recode labels\n",
    "int_to_label = {2: \"agree\", 1 : \"neutral\", 0 : \"disagree\"}\n",
    "data.replace({\"label\": int_to_label}, inplace = True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_parent</th>\n",
       "      <th>body_child</th>\n",
       "      <th>msg_id_parent</th>\n",
       "      <th>msg_id_child</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>datetime</th>\n",
       "      <th>exact_time</th>\n",
       "      <th>target</th>\n",
       "      <th>label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>I live in rural Saskatchewan, Canada. We have ...</td>\n",
       "      <td>I'm in NE USA we've had 3 in two years...all e...</td>\n",
       "      <td>cnddov1</td>\n",
       "      <td>cndj2gv</td>\n",
       "      <td>climate</td>\n",
       "      <td>03/01/2015 23:18</td>\n",
       "      <td>1420327135</td>\n",
       "      <td>0</td>\n",
       "      <td>no_disagreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>I live in rural Saskatchewan, Canada. We have ...</td>\n",
       "      <td>One hundred year flood just means a one in one...</td>\n",
       "      <td>cnddov1</td>\n",
       "      <td>cndkpy7</td>\n",
       "      <td>climate</td>\n",
       "      <td>04/01/2015 00:10</td>\n",
       "      <td>1420330231</td>\n",
       "      <td>0</td>\n",
       "      <td>no_disagreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Convince her of what? That it's happening or t...</td>\n",
       "      <td>That anthropocentric climate change is actuall...</td>\n",
       "      <td>cndnlrd</td>\n",
       "      <td>cndnsxt</td>\n",
       "      <td>climate</td>\n",
       "      <td>04/01/2015 01:45</td>\n",
       "      <td>1420335952</td>\n",
       "      <td>0</td>\n",
       "      <td>no_disagreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disagree</td>\n",
       "      <td>I think this prediction is about as valid as s...</td>\n",
       "      <td>It's January. Literally no one said it would b...</td>\n",
       "      <td>cndl5x4</td>\n",
       "      <td>cndybsy</td>\n",
       "      <td>climate</td>\n",
       "      <td>04/01/2015 08:01</td>\n",
       "      <td>1420358465</td>\n",
       "      <td>1</td>\n",
       "      <td>disagree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disagree</td>\n",
       "      <td>Mann hasn't been honest in decades, so I'm cur...</td>\n",
       "      <td>There have been a dozen re-constructions of Ma...</td>\n",
       "      <td>cne462t</td>\n",
       "      <td>cne89ej</td>\n",
       "      <td>climate</td>\n",
       "      <td>04/01/2015 17:45</td>\n",
       "      <td>1420393544</td>\n",
       "      <td>1</td>\n",
       "      <td>disagree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42838</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Not trying to spark an argument but a legitima...</td>\n",
       "      <td>Keeping in mind that the Palestinians killed m...</td>\n",
       "      <td>gyo197v</td>\n",
       "      <td>gyotff1</td>\n",
       "      <td>Republican</td>\n",
       "      <td>19/05/2021 12:36</td>\n",
       "      <td>1621427788</td>\n",
       "      <td>0</td>\n",
       "      <td>no_disagreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42839</th>\n",
       "      <td>agree</td>\n",
       "      <td>Y'all saw Guilianis hail Mary right? Get his s...</td>\n",
       "      <td>Same I want these assholes in jail, full stop....</td>\n",
       "      <td>gynfsu4</td>\n",
       "      <td>gyp3u39</td>\n",
       "      <td>democrats</td>\n",
       "      <td>19/05/2021 13:56</td>\n",
       "      <td>1621432578</td>\n",
       "      <td>0</td>\n",
       "      <td>no_disagreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42840</th>\n",
       "      <td>agree</td>\n",
       "      <td>Why don't I see ads holding Republicans accoun...</td>\n",
       "      <td>Yeah, I agree with the goal of this post but n...</td>\n",
       "      <td>gyn6nzm</td>\n",
       "      <td>gyp5vzw</td>\n",
       "      <td>democrats</td>\n",
       "      <td>19/05/2021 14:11</td>\n",
       "      <td>1621433471</td>\n",
       "      <td>0</td>\n",
       "      <td>no_disagreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42841</th>\n",
       "      <td>agree</td>\n",
       "      <td>How about ... no? This is strange. Community o...</td>\n",
       "      <td>I know, it feels strange too. We wouldn't hold...</td>\n",
       "      <td>gyp71o7</td>\n",
       "      <td>gyp7en6</td>\n",
       "      <td>BlackLivesMatter</td>\n",
       "      <td>19/05/2021 14:21</td>\n",
       "      <td>1621434116</td>\n",
       "      <td>0</td>\n",
       "      <td>no_disagreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42842</th>\n",
       "      <td>agree</td>\n",
       "      <td>Can you recruit some white allies so you aren'...</td>\n",
       "      <td>Ooooo, now I like the taking the day request. ...</td>\n",
       "      <td>gyp7b89</td>\n",
       "      <td>gyp7vjv</td>\n",
       "      <td>BlackLivesMatter</td>\n",
       "      <td>19/05/2021 14:25</td>\n",
       "      <td>1621434314</td>\n",
       "      <td>0</td>\n",
       "      <td>no_disagreement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42843 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                        body_parent  \\\n",
       "0       neutral  I live in rural Saskatchewan, Canada. We have ...   \n",
       "1       neutral  I live in rural Saskatchewan, Canada. We have ...   \n",
       "2       neutral  Convince her of what? That it's happening or t...   \n",
       "3      disagree  I think this prediction is about as valid as s...   \n",
       "4      disagree  Mann hasn't been honest in decades, so I'm cur...   \n",
       "...         ...                                                ...   \n",
       "42838   neutral  Not trying to spark an argument but a legitima...   \n",
       "42839     agree  Y'all saw Guilianis hail Mary right? Get his s...   \n",
       "42840     agree  Why don't I see ads holding Republicans accoun...   \n",
       "42841     agree  How about ... no? This is strange. Community o...   \n",
       "42842     agree  Can you recruit some white allies so you aren'...   \n",
       "\n",
       "                                              body_child msg_id_parent  \\\n",
       "0      I'm in NE USA we've had 3 in two years...all e...       cnddov1   \n",
       "1      One hundred year flood just means a one in one...       cnddov1   \n",
       "2      That anthropocentric climate change is actuall...       cndnlrd   \n",
       "3      It's January. Literally no one said it would b...       cndl5x4   \n",
       "4      There have been a dozen re-constructions of Ma...       cne462t   \n",
       "...                                                  ...           ...   \n",
       "42838  Keeping in mind that the Palestinians killed m...       gyo197v   \n",
       "42839  Same I want these assholes in jail, full stop....       gynfsu4   \n",
       "42840  Yeah, I agree with the goal of this post but n...       gyn6nzm   \n",
       "42841  I know, it feels strange too. We wouldn't hold...       gyp71o7   \n",
       "42842  Ooooo, now I like the taking the day request. ...       gyp7b89   \n",
       "\n",
       "      msg_id_child         subreddit          datetime  exact_time  target  \\\n",
       "0          cndj2gv           climate  03/01/2015 23:18  1420327135       0   \n",
       "1          cndkpy7           climate  04/01/2015 00:10  1420330231       0   \n",
       "2          cndnsxt           climate  04/01/2015 01:45  1420335952       0   \n",
       "3          cndybsy           climate  04/01/2015 08:01  1420358465       1   \n",
       "4          cne89ej           climate  04/01/2015 17:45  1420393544       1   \n",
       "...            ...               ...               ...         ...     ...   \n",
       "42838      gyotff1        Republican  19/05/2021 12:36  1621427788       0   \n",
       "42839      gyp3u39         democrats  19/05/2021 13:56  1621432578       0   \n",
       "42840      gyp5vzw         democrats  19/05/2021 14:11  1621433471       0   \n",
       "42841      gyp7en6  BlackLivesMatter  19/05/2021 14:21  1621434116       0   \n",
       "42842      gyp7vjv  BlackLivesMatter  19/05/2021 14:25  1621434314       0   \n",
       "\n",
       "               label_2  \n",
       "0      no_disagreement  \n",
       "1      no_disagreement  \n",
       "2      no_disagreement  \n",
       "3             disagree  \n",
       "4             disagree  \n",
       "...                ...  \n",
       "42838  no_disagreement  \n",
       "42839  no_disagreement  \n",
       "42840  no_disagreement  \n",
       "42841  no_disagreement  \n",
       "42842  no_disagreement  \n",
       "\n",
       "[42843 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adapt true labels\n",
    "labels_2 = []\n",
    "target_new = []\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    if row['label'] in ['neutral', 'agree']:\n",
    "        labels_2.append('no_disagreement')\n",
    "        target_new.append(0)\n",
    "    else:\n",
    "        labels_2.append('disagree')\n",
    "        target_new.append(1)\n",
    "        \n",
    "data['label_2'] = labels_2\n",
    "data['target'] = target_new\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>reply</th>\n",
       "      <th>label</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I live in rural Saskatchewan, Canada. We have ...</td>\n",
       "      <td>I'm in NE USA we've had 3 in two years...all e...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I live in rural Saskatchewan, Canada. We have ...</td>\n",
       "      <td>One hundred year flood just means a one in one...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Convince her of what? That it's happening or t...</td>\n",
       "      <td>That anthropocentric climate change is actuall...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think this prediction is about as valid as s...</td>\n",
       "      <td>It's January. Literally no one said it would b...</td>\n",
       "      <td>disagree</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mann hasn't been honest in decades, so I'm cur...</td>\n",
       "      <td>There have been a dozen re-constructions of Ma...</td>\n",
       "      <td>disagree</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42838</th>\n",
       "      <td>Not trying to spark an argument but a legitima...</td>\n",
       "      <td>Keeping in mind that the Palestinians killed m...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42839</th>\n",
       "      <td>Y'all saw Guilianis hail Mary right? Get his s...</td>\n",
       "      <td>Same I want these assholes in jail, full stop....</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42840</th>\n",
       "      <td>Why don't I see ads holding Republicans accoun...</td>\n",
       "      <td>Yeah, I agree with the goal of this post but n...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42841</th>\n",
       "      <td>How about ... no? This is strange. Community o...</td>\n",
       "      <td>I know, it feels strange too. We wouldn't hold...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42842</th>\n",
       "      <td>Can you recruit some white allies so you aren'...</td>\n",
       "      <td>Ooooo, now I like the taking the day request. ...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42843 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  \\\n",
       "0      I live in rural Saskatchewan, Canada. We have ...   \n",
       "1      I live in rural Saskatchewan, Canada. We have ...   \n",
       "2      Convince her of what? That it's happening or t...   \n",
       "3      I think this prediction is about as valid as s...   \n",
       "4      Mann hasn't been honest in decades, so I'm cur...   \n",
       "...                                                  ...   \n",
       "42838  Not trying to spark an argument but a legitima...   \n",
       "42839  Y'all saw Guilianis hail Mary right? Get his s...   \n",
       "42840  Why don't I see ads holding Republicans accoun...   \n",
       "42841  How about ... no? This is strange. Community o...   \n",
       "42842  Can you recruit some white allies so you aren'...   \n",
       "\n",
       "                                                   reply            label  \\\n",
       "0      I'm in NE USA we've had 3 in two years...all e...  no_disagreement   \n",
       "1      One hundred year flood just means a one in one...  no_disagreement   \n",
       "2      That anthropocentric climate change is actuall...  no_disagreement   \n",
       "3      It's January. Literally no one said it would b...         disagree   \n",
       "4      There have been a dozen re-constructions of Ma...         disagree   \n",
       "...                                                  ...              ...   \n",
       "42838  Keeping in mind that the Palestinians killed m...  no_disagreement   \n",
       "42839  Same I want these assholes in jail, full stop....  no_disagreement   \n",
       "42840  Yeah, I agree with the goal of this post but n...  no_disagreement   \n",
       "42841  I know, it feels strange too. We wouldn't hold...  no_disagreement   \n",
       "42842  Ooooo, now I like the taking the day request. ...  no_disagreement   \n",
       "\n",
       "       target  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           1  \n",
       "...       ...  \n",
       "42838       0  \n",
       "42839       0  \n",
       "42840       0  \n",
       "42841       0  \n",
       "42842       0  \n",
       "\n",
       "[42843 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make text\n",
    "\n",
    "def create_training_data(data):\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        #system_prompt = \"\"\"You are a classification Chatbot. Given a comment and a reply, you classify whether the reply disagrees with the comment, or not. You only reply with either \"disagree\" or \"no_disagreement\" and nothing else.\"\"\"\n",
    "        comment = row[\"body_parent\"]\n",
    "        reply = row[\"body_child\"]\n",
    "        label = row[\"label_2\"]\n",
    "        target = row[\"target\"]\n",
    "        result.append({'comment' : comment, 'reply': reply, 'label' : label, 'target' : target})\n",
    "    \n",
    "    return result\n",
    "\n",
    "# save data\n",
    "df = pd.DataFrame(create_training_data(data))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>reply</th>\n",
       "      <th>label</th>\n",
       "      <th>target</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I live in rural Saskatchewan, Canada. We have ...</td>\n",
       "      <td>I'm in NE USA we've had 3 in two years...all e...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: I live in rural Saskatchewan, Canada....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I live in rural Saskatchewan, Canada. We have ...</td>\n",
       "      <td>One hundred year flood just means a one in one...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: I live in rural Saskatchewan, Canada....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Convince her of what? That it's happening or t...</td>\n",
       "      <td>That anthropocentric climate change is actuall...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: Convince her of what? That it's happe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think this prediction is about as valid as s...</td>\n",
       "      <td>It's January. Literally no one said it would b...</td>\n",
       "      <td>disagree</td>\n",
       "      <td>1</td>\n",
       "      <td>Comment: I think this prediction is about as v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mann hasn't been honest in decades, so I'm cur...</td>\n",
       "      <td>There have been a dozen re-constructions of Ma...</td>\n",
       "      <td>disagree</td>\n",
       "      <td>1</td>\n",
       "      <td>Comment: Mann hasn't been honest in decades, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42838</th>\n",
       "      <td>Not trying to spark an argument but a legitima...</td>\n",
       "      <td>Keeping in mind that the Palestinians killed m...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: Not trying to spark an argument but a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42839</th>\n",
       "      <td>Y'all saw Guilianis hail Mary right? Get his s...</td>\n",
       "      <td>Same I want these assholes in jail, full stop....</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: Y'all saw Guilianis hail Mary right? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42840</th>\n",
       "      <td>Why don't I see ads holding Republicans accoun...</td>\n",
       "      <td>Yeah, I agree with the goal of this post but n...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: Why don't I see ads holding Republica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42841</th>\n",
       "      <td>How about ... no? This is strange. Community o...</td>\n",
       "      <td>I know, it feels strange too. We wouldn't hold...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: How about ... no? This is strange. Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42842</th>\n",
       "      <td>Can you recruit some white allies so you aren'...</td>\n",
       "      <td>Ooooo, now I like the taking the day request. ...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: Can you recruit some white allies so ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42843 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  \\\n",
       "0      I live in rural Saskatchewan, Canada. We have ...   \n",
       "1      I live in rural Saskatchewan, Canada. We have ...   \n",
       "2      Convince her of what? That it's happening or t...   \n",
       "3      I think this prediction is about as valid as s...   \n",
       "4      Mann hasn't been honest in decades, so I'm cur...   \n",
       "...                                                  ...   \n",
       "42838  Not trying to spark an argument but a legitima...   \n",
       "42839  Y'all saw Guilianis hail Mary right? Get his s...   \n",
       "42840  Why don't I see ads holding Republicans accoun...   \n",
       "42841  How about ... no? This is strange. Community o...   \n",
       "42842  Can you recruit some white allies so you aren'...   \n",
       "\n",
       "                                                   reply            label  \\\n",
       "0      I'm in NE USA we've had 3 in two years...all e...  no_disagreement   \n",
       "1      One hundred year flood just means a one in one...  no_disagreement   \n",
       "2      That anthropocentric climate change is actuall...  no_disagreement   \n",
       "3      It's January. Literally no one said it would b...         disagree   \n",
       "4      There have been a dozen re-constructions of Ma...         disagree   \n",
       "...                                                  ...              ...   \n",
       "42838  Keeping in mind that the Palestinians killed m...  no_disagreement   \n",
       "42839  Same I want these assholes in jail, full stop....  no_disagreement   \n",
       "42840  Yeah, I agree with the goal of this post but n...  no_disagreement   \n",
       "42841  I know, it feels strange too. We wouldn't hold...  no_disagreement   \n",
       "42842  Ooooo, now I like the taking the day request. ...  no_disagreement   \n",
       "\n",
       "       target                                             prompt  \n",
       "0           0  Comment: I live in rural Saskatchewan, Canada....  \n",
       "1           0  Comment: I live in rural Saskatchewan, Canada....  \n",
       "2           0  Comment: Convince her of what? That it's happe...  \n",
       "3           1  Comment: I think this prediction is about as v...  \n",
       "4           1  Comment: Mann hasn't been honest in decades, s...  \n",
       "...       ...                                                ...  \n",
       "42838       0  Comment: Not trying to spark an argument but a...  \n",
       "42839       0  Comment: Y'all saw Guilianis hail Mary right? ...  \n",
       "42840       0  Comment: Why don't I see ads holding Republica...  \n",
       "42841       0  Comment: How about ... no? This is strange. Co...  \n",
       "42842       0  Comment: Can you recruit some white allies so ...  \n",
       "\n",
       "[42843 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['prompt'] = None\n",
    "\n",
    "def make_prompt(row):\n",
    "\n",
    "    prompt = \"Comment: \" + row[\"comment\"] + \"; Reply: \" + row[\"reply\"]\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "df['prompt'] = df.apply(lambda row: make_prompt(row), axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "Make train/val/test split by time order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame\n",
    "train_size = 0.8\n",
    "eval_size = 0.1\n",
    "\n",
    "# Determine splitting indexes (ordered by time)\n",
    "train_end = int(train_size * len(df))\n",
    "eval_end = train_end + int(eval_size * len(df))\n",
    "\n",
    "# Split the data\n",
    "X_train = df[:train_end]\n",
    "X_eval = df[train_end:eval_end]\n",
    "X_test = df[eval_end:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert from Pandas DataFrame to Hugging Face Dataset\n",
    "* Also let's shuffle the training set.\n",
    "* We put the components train,val,test into a DatasetDict so we can access them later with HF trainer.\n",
    "* Later we will add a tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['comment', 'reply', 'target', 'prompt'],\n",
       "    num_rows: 4285\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dataset = Dataset.from_pandas(X_train.drop('label', axis = 1))\n",
    "X_eval_dataset = Dataset.from_pandas(X_eval.drop('label', axis = 1))\n",
    "X_test_dataset = Dataset.from_pandas(X_test.drop('label', axis = 1))\n",
    "\n",
    "X_test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle training data --> apparently this helps with performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset_shuffle = X_train_dataset.shuffle(seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['comment', 'reply', 'target', 'prompt'],\n",
       "        num_rows: 34274\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['comment', 'reply', 'target', 'prompt'],\n",
       "        num_rows: 4284\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['comment', 'reply', 'target', 'prompt'],\n",
       "        num_rows: 4285\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DatasetDict({\n",
    "    'train' : X_train_dataset_shuffle,\n",
    "    'val' : X_eval_dataset,\n",
    "    'test' : X_test_dataset\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    0.599463\n",
       "1    0.400537\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.target.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weights\n",
    "\n",
    "* Since our classes are not balanced let's calculate class weights based on inverse value counts\n",
    "* Convert to pytorch tensor since we will need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4005, 0.5995])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invert the weights\n",
    "class_weights = (1/X_train.target.value_counts(normalize = True).sort_index()).to_list()\n",
    "\n",
    "# make a tensor\n",
    "class_weights = torch.tensor(class_weights)\n",
    "\n",
    "# make them sum to one\n",
    "class_weights = class_weights/class_weights.sum()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Load the Model**\n",
    "\n",
    "Apparently, meta recommends the base version of the model for finetuning [source](https://www.youtube.com/watch?v=YJNbgusTSF0)\n",
    "\n",
    "* load model with 4bit quantization (as specified in bits and bytes) and prepare model for peft training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization for QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, # enable 4 bit quantization\n",
    "    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n",
    "    bnb_4bit_use_double_quant = True, # quantize quantized weights\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 16, # dimensions of low-rank matrices\n",
    "    lora_alpha = 8, # scaling factor (trade-off) for LoRA activations vs. pretrained weight activations\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], # where to apply LoRA to\n",
    "    lora_dropout = 0.05, # drop out probability of LoRA layers, to prevent overfitting\n",
    "    bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n",
    "    task_type = 'SEQ_CLS'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model\n",
    "\n",
    "* AutoModelForSequenceClassification --> used for classifications\n",
    "* Num of labels is # of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b121d89128c447cc8441cd0e31e19621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.1-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (score): Linear(in_features=4096, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = quantization_config,\n",
    "    num_labels = 2\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to preprocess quantized model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (score): Linear(in_features=4096, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for PEFT training\n",
    "\n",
    "`get_peft_model()` to prepare the model for training with PEFT method such as LoRA by wrapping the base model and PEFT configuration with `get_peft_model()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tokenizer**\n",
    "\n",
    "### Since LLAMA3 pre-training doesn't have EOS token\n",
    "* Set the pad_token_id to eos_token_id\n",
    "* Set pad token ot eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space = True)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Update Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_pt = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Apply model to get performance prior to training\n",
    "\n",
    "**Useless, as the model will output random logits $ßto$ it has not learned anything yet**\n",
    "\n",
    "* Use batch size 32 to vectorize and avoid memory errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Comment: luckily they won't retake the house. Has this piece of shit ever been on any other news channel other than FoxNews, OAN, and NewsMax? This man only speaks and care about his base, he doesn't give a shit about the rest of America.; Reply: I hope you're right but given that the GOP made gains in 2020, let's not be too overconfident 2022 won't have a red wave.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[100]['prompt'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 7/537 [00:03<04:21,  2.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(batch_sentences, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Move tensors to the device where the model is (e.g., GPU or CPU)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Perform inference and store the logits\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[0;32mIn[28], line 21\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(batch_sentences, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Move tensors to the device where the model is (e.g., GPU or CPU)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Perform inference and store the logits\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Convert summaries to a list\n",
    "sentences = X_test.prompt.tolist()\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 8  # You can adjust this based on your system's memory capacity\n",
    "\n",
    "# Initialize an empty list to store the model outputs\n",
    "all_outputs = []\n",
    "\n",
    "# Process the sentences in batches\n",
    "for i in tqdm(range(0, len(sentences), batch_size)):\n",
    "    # Get the batch of sentences\n",
    "    batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "    # Tokenize the batch\n",
    "    inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Move tensors to the device where the model is (e.g., GPU or CPU)\n",
    "    inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
    "\n",
    "    # Perform inference and store the logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        all_outputs.append(outputs['logits'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate Outputs into a single tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4675, -2.4747],\n",
       "        [ 2.0257,  0.0061],\n",
       "        [ 0.1863,  0.3972],\n",
       "        ...,\n",
       "        [ 4.6069, -1.0798],\n",
       "        [ 1.2376, -2.0870],\n",
       "        [ 2.0624, -0.1581]], device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_outputs = torch.cat(all_outputs, dim=0)\n",
    "final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get **probabilities** with softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5018, 0.4982],\n",
      "        [0.8828, 0.1172],\n",
      "        [0.4475, 0.5525],\n",
      "        ...,\n",
      "        [0.9966, 0.0034],\n",
      "        [0.9653, 0.0347],\n",
      "        [0.9021, 0.0979]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "probabilities = F.softmax(final_outputs, dim=1)\n",
    "print(probabilities) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = probabilities.argmax(dim=1)  # Get class labels\n",
    "certainty_scores = probabilities.max(dim=1).values  # Get max probability per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5018, 0.8828, 0.5525,  ..., 0.9966, 0.9653, 0.9021], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels\n",
    "certainty_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract predictions and turn to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_882/1638758842.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['predictions_label_initial'] = predicted_labels.cpu().numpy()\n",
      "/tmp/ipykernel_882/1638758842.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['predictions_score_initial'] = certainty_scores.cpu().numpy()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>reply</th>\n",
       "      <th>label</th>\n",
       "      <th>target</th>\n",
       "      <th>prompt</th>\n",
       "      <th>predictions_label_initial</th>\n",
       "      <th>predictions_score_initial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38604</th>\n",
       "      <td>It's so nice having a FLOTUS who's facial expr...</td>\n",
       "      <td>Melanoma's squinty cat face always looked to m...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: It's so nice having a FLOTUS who's fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.501805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38605</th>\n",
       "      <td>Because Mitch McConnell indicated he's voting ...</td>\n",
       "      <td>I think it's worth it because the more we air ...</td>\n",
       "      <td>disagree</td>\n",
       "      <td>1</td>\n",
       "      <td>Comment: Because Mitch McConnell indicated he'...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.882832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38606</th>\n",
       "      <td>How about some stimulus checks and a decent st...</td>\n",
       "      <td>You get this was an executive action, not legi...</td>\n",
       "      <td>disagree</td>\n",
       "      <td>1</td>\n",
       "      <td>Comment: How about some stimulus checks and a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.552526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38607</th>\n",
       "      <td>Satire feels appropriate. I'd like one dose of...</td>\n",
       "      <td>Are you saying they didn't know or understand ...</td>\n",
       "      <td>disagree</td>\n",
       "      <td>1</td>\n",
       "      <td>Comment: Satire feels appropriate. I'd like on...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.909323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38608</th>\n",
       "      <td>I actually didn't want to upload these particu...</td>\n",
       "      <td>To be fair they are just reporting what Brexit...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: I actually didn't want to upload thes...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.911840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42889</th>\n",
       "      <td>Not trying to spark an argument but a legitima...</td>\n",
       "      <td>Keeping in mind that the Palestinians killed m...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: Not trying to spark an argument but a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.905063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42890</th>\n",
       "      <td>Y'all saw Guilianis hail Mary right? Get his s...</td>\n",
       "      <td>Same I want these assholes in jail, full stop....</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: Y'all saw Guilianis hail Mary right? ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.973733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42891</th>\n",
       "      <td>&gt;Why don't I see ads holding Republicans accou...</td>\n",
       "      <td>Yeah, I agree with the goal of this post but n...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: &gt;Why don't I see ads holding Republic...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42892</th>\n",
       "      <td>How about ... no? This is strange. Community o...</td>\n",
       "      <td>I know, it feels strange too. We wouldn't hold...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: How about ... no? This is strange. Co...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.965263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42893</th>\n",
       "      <td>Can you recruit some white allies so you aren'...</td>\n",
       "      <td>Ooooo, now I like the taking the day request. ...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: Can you recruit some white allies so ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.902083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4290 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  \\\n",
       "38604  It's so nice having a FLOTUS who's facial expr...   \n",
       "38605  Because Mitch McConnell indicated he's voting ...   \n",
       "38606  How about some stimulus checks and a decent st...   \n",
       "38607  Satire feels appropriate. I'd like one dose of...   \n",
       "38608  I actually didn't want to upload these particu...   \n",
       "...                                                  ...   \n",
       "42889  Not trying to spark an argument but a legitima...   \n",
       "42890  Y'all saw Guilianis hail Mary right? Get his s...   \n",
       "42891  >Why don't I see ads holding Republicans accou...   \n",
       "42892  How about ... no? This is strange. Community o...   \n",
       "42893  Can you recruit some white allies so you aren'...   \n",
       "\n",
       "                                                   reply            label  \\\n",
       "38604  Melanoma's squinty cat face always looked to m...  no_disagreement   \n",
       "38605  I think it's worth it because the more we air ...         disagree   \n",
       "38606  You get this was an executive action, not legi...         disagree   \n",
       "38607  Are you saying they didn't know or understand ...         disagree   \n",
       "38608  To be fair they are just reporting what Brexit...  no_disagreement   \n",
       "...                                                  ...              ...   \n",
       "42889  Keeping in mind that the Palestinians killed m...  no_disagreement   \n",
       "42890  Same I want these assholes in jail, full stop....  no_disagreement   \n",
       "42891  Yeah, I agree with the goal of this post but n...  no_disagreement   \n",
       "42892  I know, it feels strange too. We wouldn't hold...  no_disagreement   \n",
       "42893  Ooooo, now I like the taking the day request. ...  no_disagreement   \n",
       "\n",
       "       target                                             prompt  \\\n",
       "38604       0  Comment: It's so nice having a FLOTUS who's fa...   \n",
       "38605       1  Comment: Because Mitch McConnell indicated he'...   \n",
       "38606       1  Comment: How about some stimulus checks and a ...   \n",
       "38607       1  Comment: Satire feels appropriate. I'd like on...   \n",
       "38608       0  Comment: I actually didn't want to upload thes...   \n",
       "...       ...                                                ...   \n",
       "42889       0  Comment: Not trying to spark an argument but a...   \n",
       "42890       0  Comment: Y'all saw Guilianis hail Mary right? ...   \n",
       "42891       0  Comment: >Why don't I see ads holding Republic...   \n",
       "42892       0  Comment: How about ... no? This is strange. Co...   \n",
       "42893       0  Comment: Can you recruit some white allies so ...   \n",
       "\n",
       "       predictions_label_initial  predictions_score_initial  \n",
       "38604                          0                   0.501805  \n",
       "38605                          0                   0.882832  \n",
       "38606                          1                   0.552526  \n",
       "38607                          0                   0.909323  \n",
       "38608                          0                   0.911840  \n",
       "...                          ...                        ...  \n",
       "42889                          0                   0.905063  \n",
       "42890                          0                   0.973733  \n",
       "42891                          0                   0.996621  \n",
       "42892                          0                   0.965263  \n",
       "42893                          0                   0.902083  \n",
       "\n",
       "[4290 rows x 7 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['predictions_label_initial'] = predicted_labels.cpu().numpy()\n",
    "X_test['predictions_score_initial'] = certainty_scores.cpu().numpy()\n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'predictions_label_initial'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'predictions_label_initial'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m label_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_disagreement\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisagree\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m----> 3\u001b[0m X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions_initial\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredictions_label_initial\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m l:label_dict[l])\n\u001b[1;32m      4\u001b[0m X_test\u001b[38;5;66;03m#['predictions_initial']\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3896\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3898\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'predictions_label_initial'"
     ]
    }
   ],
   "source": [
    "label_dict = {0 : 'no_disagreement', 1: 'disagree'}\n",
    "\n",
    "X_test['predictions_initial']=X_test['predictions_label_initial'].apply(lambda l:label_dict[l])\n",
    "X_test#['predictions_initial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions_initial\n",
       "no_disagreement    3788\n",
       "disagree            502\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['predictions_initial'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(\"output/v4_Llama_3.1_8B_initial_X_test_2labels_scale.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Three versions of initial performance**\n",
    "\n",
    "$\\to$ has not learned anything yet, so random!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "no_disagreement    2666\n",
       "disagree           1624\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_metrics(df_test, pred_col):\n",
    "  y_test = df_test.label\n",
    "  y_pred = df_test[pred_col]\n",
    "\n",
    "  print(\"Confusion Matrix:\")\n",
    "  print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "  print(\"\\nClassification Report:\")\n",
    "  print(classification_report(y_test, y_pred))\n",
    "\n",
    "  print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
    "  print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test_old' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_performance_metrics(\u001b[43mX_test_old\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions_initial\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test_old' is not defined"
     ]
    }
   ],
   "source": [
    "get_performance_metrics(X_test; 'predictions_initial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Trainer**\n",
    "\n",
    "* model\n",
    "* tokenizer\n",
    "* training arguments\n",
    "* train dataset\n",
    "* eval dataset\n",
    "* Data Collater\n",
    "* Compute Metrics\n",
    "* class_weights: In our case since we are using a custom trainer so we can use a weighted loss we will subclass trainer and define the custom loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LLAMA tokenized dataset which will house our train/val parts during the training process but after applying tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['comment', 'reply', 'target', 'prompt'],\n",
       "        num_rows: 34274\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['comment', 'reply', 'target', 'prompt'],\n",
       "        num_rows: 4284\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['comment', 'reply', 'target', 'prompt'],\n",
       "        num_rows: 4285\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718fccbeeb624e7f9b80cfd86abb4688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34274 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f8f1c3b08b4874b6cb9d940e676d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26c067305854403905a11054d72f52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "col_to_delete = ['comment', 'reply', 'prompt']\n",
    "\n",
    "def llama_preprocessing_function(examples):\n",
    "    return tokenizer(examples['prompt'], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "tokenized_datasets = dataset.map(llama_preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 34274\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4284\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4285\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collator\n",
    "A **data collator** prepares batches of data for training or inference in machine learning, ensuring uniform formatting and adherence to model input requirements. This is especially crucial for variable-sized inputs like text sequences.\n",
    "\n",
    "### Functions of Data Collator\n",
    "\n",
    "1. **Padding:** Uniformly pads sequences to the length of the longest sequence using a special token, allowing simultaneous batch processing.\n",
    "2. **Batching:** Groups individual data points into batches for efficient processing.\n",
    "3. **Handling Special Tokens:** Adds necessary special tokens to sequences.\n",
    "4. **Converting to Tensor:** Transforms data into tensors, the required format for machine learning frameworks.\n",
    "\n",
    "### `DataCollatorWithPadding`\n",
    "\n",
    "The `DataCollatorWithPadding` specifically manages padding, using a tokenizer to ensure that all sequences are padded to the same length for consistent model input.\n",
    "\n",
    "- **Syntax:** `collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)`\n",
    "- **Purpose:** Automatically pads text data to the longest sequence in a batch, crucial for models like BERT or GPT.\n",
    "- **Tokenizer:** Uses the provided `tokenizer` for sequence processing, respecting model-specific vocabulary and formatting rules.\n",
    "\n",
    "This collator is commonly used with libraries like Hugging Face's Transformers, facilitating data preprocessing for various NLP models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'balanced_accuracy' : balanced_accuracy_score(predictions, labels),'accuracy':accuracy_score(predictions,labels)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Custom Trainer\n",
    "* We will have a custom loss function that deals with the class weights and have class weights as additional argument in constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Ensure label_weights is a tensor\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Extract labels and convert them to long type for cross_entropy\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract logits assuming they are directly outputted by the model\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        # Compute custom loss with class weights for imbalanced data handling\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "***\n",
    "## **Training** --> 5 epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = 'Llama_3.1_8B_training_2labels_scale_FFN',\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 5, # checked up to 5, but overfitting\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_882/37245773.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "/tmp/ipykernel_882/37245773.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_datasets['train'],\n",
    "    eval_dataset = tokenized_datasets['val'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics = compute_metrics,\n",
    "    class_weights=class_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Run the Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/llama/ClassificationLM/Llama_3.1_8B_2labels_scale/wandb/run-20250325_083626-p10fy2pl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/elena-solar-university-of-konstanz/huggingface/runs/p10fy2pl' target=\"_blank\">Llama_3.1_8B_training_2labels_scale</a></strong> to <a href='https://wandb.ai/elena-solar-university-of-konstanz/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/elena-solar-university-of-konstanz/huggingface' target=\"_blank\">https://wandb.ai/elena-solar-university-of-konstanz/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/elena-solar-university-of-konstanz/huggingface/runs/p10fy2pl' target=\"_blank\">https://wandb.ai/elena-solar-university-of-konstanz/huggingface/runs/p10fy2pl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21450' max='21450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21450/21450 8:32:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.464300</td>\n",
       "      <td>0.443172</td>\n",
       "      <td>0.812316</td>\n",
       "      <td>0.821870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.418500</td>\n",
       "      <td>0.446552</td>\n",
       "      <td>0.815787</td>\n",
       "      <td>0.826533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.367100</td>\n",
       "      <td>0.547217</td>\n",
       "      <td>0.808032</td>\n",
       "      <td>0.820937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.306700</td>\n",
       "      <td>0.704391</td>\n",
       "      <td>0.810855</td>\n",
       "      <td>0.823735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.206600</td>\n",
       "      <td>0.952339</td>\n",
       "      <td>0.808707</td>\n",
       "      <td>0.820937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =          5.0\n",
      "  total_flos               = 1021181669GF\n",
      "  train_loss               =       0.3661\n",
      "  train_runtime            =   8:32:26.73\n",
      "  train_samples            =        34315\n",
      "  train_samples_per_second =         5.58\n",
      "  train_steps_per_second   =        0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "\n",
    "# metrics\n",
    "metrics = train_result.metrics\n",
    "max_train_samples = len(X_train)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(X_train))\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "trainer.model.eval()\n",
    "# save\n",
    "trainer.model.save_pretrained(\"Llama_3.1_8B_saved_model_2labels_scale\")\n",
    "\n",
    "trainer.tokenizer.save_pretrained(\"Llama_3.1_8B_saved_model_2labels_scale\")\n",
    "trainer.model.config.save_pretrained(\"Llama_3.1_8B_saved_model_2labels_scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **Training** --> 2 epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = 'final_Llama_3.1_8B_training_2labels_scale_FFN',\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 2, # checked up to 5, but overfitting\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5808/37245773.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "/tmp/ipykernel_5808/37245773.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_datasets['train'],\n",
    "    eval_dataset = tokenized_datasets['val'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics = compute_metrics,\n",
    "    class_weights=class_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Run the Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1091' max='8570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1091/8570 1:16:40 < 8:46:37, 0.24 it/s, Epoch 0.25/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "\n",
    "# metrics\n",
    "metrics = train_result.metrics\n",
    "max_train_samples = len(X_train)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(X_train))\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "# set to evaulation mode\n",
    "trainer.model.eval() \n",
    "\n",
    "# save LoRa adapters\n",
    "trainer.model.save_pretrained(\"final_Llama_3.1_8B_saved_model_2labels_scale_FFN\")\n",
    "\n",
    "tokenizer.save_pretrained(\"final_Llama_3.1_8B_saved_model_2labels_scale_FFN\")\n",
    "trainer.model.config.save_pretrained(\"final_Llama_3.1_8B_saved_model_2labels_scale_FFN\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.model.eval()\n",
    "\n",
    "# save LoRa adapters\n",
    "#trainer.model.save_pretrained(\"final_Llama_3.1_8B_saved_model_2labels_scale\")\n",
    "\n",
    "#trainer.processing_class.save_pretrained(\"final_Llama_3.1_8B_saved_model_2labels_scale\")\n",
    "#trainer.model.config.save_pretrained(\"final_Llama_3.1_8B_saved_model_2labels_scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         2.0\n",
      "  total_flos               = 409156285GF\n",
      "  train_loss               =      0.3899\n",
      "  train_runtime            =  3:25:17.47\n",
      "  train_samples            =       34315\n",
      "  train_samples_per_second =       5.572\n",
      "  train_steps_per_second   =       0.697\n"
     ]
    }
   ],
   "source": [
    "metrics = train_result.metrics\n",
    "max_train_samples = len(X_train)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(X_train))\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# **Reimport**\n",
    "\n",
    "1. base mode\n",
    "2. then fin etuned adapters\n",
    "3. merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization for QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, # enable 4 bit quantization\n",
    "    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n",
    "    bnb_4bit_use_double_quant = True, # quantize quantized weights\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a8630cd57f4b7382fad666c1b5d98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.1-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#base_model = model_name # already defined above \n",
    "adapter_path = \"final_Llama_3.1_8B_saved_model_2labels_scale_FFN\"\n",
    "\n",
    "# Load the base model first\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = quantization_config\n",
    ")\n",
    "\n",
    "# Load the LoRA adapter on top of it\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"final_Llama_3.1_8B_saved_model_2labels_scale_FFN\", add_prefix_space=True)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id  \n",
    "model.config.use_cache = False \n",
    "model.config.pretraining_pt = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move model to gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "model.to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:50<00:00,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3335, 0.6665],\n",
      "        [0.9175, 0.0826],\n",
      "        [0.3591, 0.6411],\n",
      "        ...,\n",
      "        [0.9536, 0.0464],\n",
      "        [0.9854, 0.0144],\n",
      "        [0.8599, 0.1401]], device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_315/524938204.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['predictions_label_ft'] = predicted_labels.cpu().numpy()\n",
      "/tmp/ipykernel_315/524938204.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['predictions_score_ft'] = certainty_scores.cpu().numpy()\n",
      "/tmp/ipykernel_315/524938204.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['predictions_ft'] = df_test['predictions_label_ft'].apply(lambda l:label_dict[l])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>reply</th>\n",
       "      <th>label</th>\n",
       "      <th>target</th>\n",
       "      <th>prompt</th>\n",
       "      <th>predictions_label_ft</th>\n",
       "      <th>predictions_score_ft</th>\n",
       "      <th>predictions_ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38558</th>\n",
       "      <td>How about some stimulus checks and a decent st...</td>\n",
       "      <td>President Biden is actually doing his job to p...</td>\n",
       "      <td>disagree</td>\n",
       "      <td>1</td>\n",
       "      <td>Comment: How about some stimulus checks and a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666504</td>\n",
       "      <td>disagree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38559</th>\n",
       "      <td>It's so nice having a FLOTUS who's facial expr...</td>\n",
       "      <td>Melanoma's squinty cat face always looked to m...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: It's so nice having a FLOTUS who's fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.917480</td>\n",
       "      <td>no_disagreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38560</th>\n",
       "      <td>Because Mitch McConnell indicated he's voting ...</td>\n",
       "      <td>I think it's worth it because the more we air ...</td>\n",
       "      <td>disagree</td>\n",
       "      <td>1</td>\n",
       "      <td>Comment: Because Mitch McConnell indicated he'...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.641113</td>\n",
       "      <td>disagree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38561</th>\n",
       "      <td>How about some stimulus checks and a decent st...</td>\n",
       "      <td>You get this was an executive action, not legi...</td>\n",
       "      <td>disagree</td>\n",
       "      <td>1</td>\n",
       "      <td>Comment: How about some stimulus checks and a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.807129</td>\n",
       "      <td>disagree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38562</th>\n",
       "      <td>Satire feels appropriate. I'd like one dose of...</td>\n",
       "      <td>Are you saying they didn't know or understand ...</td>\n",
       "      <td>disagree</td>\n",
       "      <td>1</td>\n",
       "      <td>Comment: Satire feels appropriate. I'd like on...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.765137</td>\n",
       "      <td>disagree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42838</th>\n",
       "      <td>Not trying to spark an argument but a legitima...</td>\n",
       "      <td>Keeping in mind that the Palestinians killed m...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: Not trying to spark an argument but a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.910645</td>\n",
       "      <td>disagree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42839</th>\n",
       "      <td>Y'all saw Guilianis hail Mary right? Get his s...</td>\n",
       "      <td>Same I want these assholes in jail, full stop....</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: Y'all saw Guilianis hail Mary right? ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.957031</td>\n",
       "      <td>no_disagreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42840</th>\n",
       "      <td>Why don't I see ads holding Republicans accoun...</td>\n",
       "      <td>Yeah, I agree with the goal of this post but n...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: Why don't I see ads holding Republica...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.953613</td>\n",
       "      <td>no_disagreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42841</th>\n",
       "      <td>How about ... no? This is strange. Community o...</td>\n",
       "      <td>I know, it feels strange too. We wouldn't hold...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: How about ... no? This is strange. Co...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.985352</td>\n",
       "      <td>no_disagreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42842</th>\n",
       "      <td>Can you recruit some white allies so you aren'...</td>\n",
       "      <td>Ooooo, now I like the taking the day request. ...</td>\n",
       "      <td>no_disagreement</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment: Can you recruit some white allies so ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.859863</td>\n",
       "      <td>no_disagreement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4285 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  \\\n",
       "38558  How about some stimulus checks and a decent st...   \n",
       "38559  It's so nice having a FLOTUS who's facial expr...   \n",
       "38560  Because Mitch McConnell indicated he's voting ...   \n",
       "38561  How about some stimulus checks and a decent st...   \n",
       "38562  Satire feels appropriate. I'd like one dose of...   \n",
       "...                                                  ...   \n",
       "42838  Not trying to spark an argument but a legitima...   \n",
       "42839  Y'all saw Guilianis hail Mary right? Get his s...   \n",
       "42840  Why don't I see ads holding Republicans accoun...   \n",
       "42841  How about ... no? This is strange. Community o...   \n",
       "42842  Can you recruit some white allies so you aren'...   \n",
       "\n",
       "                                                   reply            label  \\\n",
       "38558  President Biden is actually doing his job to p...         disagree   \n",
       "38559  Melanoma's squinty cat face always looked to m...  no_disagreement   \n",
       "38560  I think it's worth it because the more we air ...         disagree   \n",
       "38561  You get this was an executive action, not legi...         disagree   \n",
       "38562  Are you saying they didn't know or understand ...         disagree   \n",
       "...                                                  ...              ...   \n",
       "42838  Keeping in mind that the Palestinians killed m...  no_disagreement   \n",
       "42839  Same I want these assholes in jail, full stop....  no_disagreement   \n",
       "42840  Yeah, I agree with the goal of this post but n...  no_disagreement   \n",
       "42841  I know, it feels strange too. We wouldn't hold...  no_disagreement   \n",
       "42842  Ooooo, now I like the taking the day request. ...  no_disagreement   \n",
       "\n",
       "       target                                             prompt  \\\n",
       "38558       1  Comment: How about some stimulus checks and a ...   \n",
       "38559       0  Comment: It's so nice having a FLOTUS who's fa...   \n",
       "38560       1  Comment: Because Mitch McConnell indicated he'...   \n",
       "38561       1  Comment: How about some stimulus checks and a ...   \n",
       "38562       1  Comment: Satire feels appropriate. I'd like on...   \n",
       "...       ...                                                ...   \n",
       "42838       0  Comment: Not trying to spark an argument but a...   \n",
       "42839       0  Comment: Y'all saw Guilianis hail Mary right? ...   \n",
       "42840       0  Comment: Why don't I see ads holding Republica...   \n",
       "42841       0  Comment: How about ... no? This is strange. Co...   \n",
       "42842       0  Comment: Can you recruit some white allies so ...   \n",
       "\n",
       "       predictions_label_ft  predictions_score_ft   predictions_ft  \n",
       "38558                     1              0.666504         disagree  \n",
       "38559                     0              0.917480  no_disagreement  \n",
       "38560                     1              0.641113         disagree  \n",
       "38561                     1              0.807129         disagree  \n",
       "38562                     1              0.765137         disagree  \n",
       "...                     ...                   ...              ...  \n",
       "42838                     1              0.910645         disagree  \n",
       "42839                     0              0.957031  no_disagreement  \n",
       "42840                     0              0.953613  no_disagreement  \n",
       "42841                     0              0.985352  no_disagreement  \n",
       "42842                     0              0.859863  no_disagreement  \n",
       "\n",
       "[4285 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionary for conversions\n",
    "label_dict = {1: \"disagree\", 0 : \"no_disagreement\"}\n",
    "\n",
    "\n",
    "def make_predictions(model,df_test):\n",
    "    \n",
    "    model.eval()\n",
    "    # Convert summaries to a list\n",
    "    sentences = df_test.prompt.tolist()\n",
    "    \n",
    "    # Define the batch size\n",
    "    batch_size = 32  # You can adjust this based on your system's memory capacity\n",
    "    \n",
    "    # Initialize an empty list to store the model outputs\n",
    "    all_outputs = []\n",
    "    \n",
    "    # Process the sentences in batches\n",
    "    for i in tqdm(range(0, len(sentences), batch_size)):\n",
    "      # Get the batch of sentences\n",
    "      batch_sentences = sentences[i:i + batch_size]\n",
    "    \n",
    "      # Tokenize the batch\n",
    "      inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding = True, truncation=True, max_length=512) #return_tensors=\"pt\", padding=True,\n",
    "    \n",
    "      # Move tensors to the device where the model is (e.g., GPU or CPU)\n",
    "      inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
    "    \n",
    "      # Perform inference and store the logits\n",
    "      with torch.no_grad():\n",
    "          outputs = model(**inputs)\n",
    "          all_outputs.append(outputs['logits'])\n",
    "          \n",
    "          \n",
    "    final_outputs = torch.cat(all_outputs, dim=0)\n",
    "    probabilities = F.softmax(final_outputs, dim=1)\n",
    "    print(probabilities) \n",
    "    predicted_labels = probabilities.argmax(dim=1)  \n",
    "    certainty_scores = probabilities.max(dim=1).values \n",
    "    \n",
    "    df_test['predictions_label_ft'] = predicted_labels.cpu().numpy()\n",
    "    df_test['predictions_score_ft'] = certainty_scores.cpu().numpy()\n",
    "    df_test['predictions_ft'] = df_test['predictions_label_ft'].apply(lambda l:label_dict[l])\n",
    "    return df_test \n",
    "  \n",
    "\n",
    "\n",
    "make_predictions(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(\"output/Llama_3.1_8B_ft_X_test_2labels_scale_FFN.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"output/Llama_3.1_8B_ft_X_test_2labels_scale_FFN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_metrics(df_test, pred_col):\n",
    "  y_test = df_test.label\n",
    "  y_pred = df_test[pred_col]\n",
    "\n",
    "  print(\"Confusion Matrix:\")\n",
    "  print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "  print(\"\\nClassification Report:\")\n",
    "  print(classification_report(y_test, y_pred))\n",
    "\n",
    "  print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
    "  print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1350  274]\n",
      " [ 622 2039]]\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       disagree       0.68      0.83      0.75      1624\n",
      "no_disagreement       0.88      0.77      0.82      2661\n",
      "\n",
      "       accuracy                           0.79      4285\n",
      "      macro avg       0.78      0.80      0.79      4285\n",
      "   weighted avg       0.81      0.79      0.79      4285\n",
      "\n",
      "Balanced Accuracy Score: 0.7987670382074223\n",
      "Accuracy Score: 0.7908984830805135\n"
     ]
    }
   ],
   "source": [
    "get_performance_metrics(X_test, 'predictions_ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Classification Report per Label}\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & precision & recall & f1-score & support \\\\\n",
      "\\midrule\n",
      "disagree & 0.68 & 0.83 & 0.75 & 1624.00 \\\\\n",
      "no_disagreement & 0.88 & 0.77 & 0.82 & 2661.00 \\\\\n",
      "accuracy & 0.79 & 0.79 & 0.79 & 0.79 \\\\\n",
      "macro avg & 0.78 & 0.80 & 0.79 & 4285.00 \\\\\n",
      "weighted avg & 0.81 & 0.79 & 0.79 & 4285.00 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test = X_test.label\n",
    "y_pred = X_test['predictions_ft']\n",
    "\n",
    "report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "df = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "# Export to LaTeX table\n",
    "latex_code = df.to_latex(float_format=\"%.2f\", index=True, caption=\"Classification Report per Label\")\n",
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.1-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128001,\n",
      "  \"pretraining_pt\": 1,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare the model config\n",
    "print(model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6076443,
     "sourceId": 9893468,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 91102,
     "modelInstanceId": 68809,
     "sourceId": 104449,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 91102,
     "modelInstanceId": 68812,
     "sourceId": 108555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
